{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7a78be-978f-4171-ac0b-86d96c08e3d8",
   "metadata": {},
   "source": [
    "# Joke classification\n",
    "\n",
    "In this notebook, we detail the different steps for classifying the jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b481f2c2-756f-404f-8390-f8c0fe02086b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a173e253-6fa5-43c6-98f1-493fa47f7cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e989403-daec-46ad-a891-a5a83199bb58",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce21456-95f9-4d25-a5fa-463f8a4e3b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3013</td>\n",
       "      <td>Markin' around The Christmas Tree\\nWhat a dogg...</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12808</td>\n",
       "      <td>Yo mama so fat when jumps up in the air she ge...</td>\n",
       "      <td>Animal</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11887</td>\n",
       "      <td>Laws of Feline Physics II\\r\\n\\r\\nLaw of Dinner...</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               body category  lengths\n",
       "0   3013  Markin' around The Christmas Tree\\nWhat a dogg...   Animal     1022\n",
       "1  12808  Yo mama so fat when jumps up in the air she ge...   Animal       55\n",
       "2  11887  Laws of Feline Physics II\\r\\n\\r\\nLaw of Dinner...   Animal     1098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we saved balanced dataframe created in the Data Analysis notebook, we will use it\n",
    "df = pd.read_csv('created_dataframes/df_balanced.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3f68b-f88c-45de-93b0-4dbd64ecaec8",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f839d76-8708-490c-99d1-25935870c0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke:\n",
      " Please answer yes or no to this question.\n",
      "\n",
      "Is your answer \"no\"? \n",
      "\n",
      "Hint: This is under trick, remember. \n",
      "\n",
      "Answer: Yes or no.\n",
      "-----------------------\n",
      "After removing noisy characters:  please answer yes or no to this question is your answer no hint this is under trick remember answer yes or no\n",
      "After removing stopwords:  please answer yes question answer hint trick remember answer yes\n",
      "Tokenized: ['please', 'answer', 'yes', 'question', 'answer', 'hint', 'trick', 'remember', 'answer', 'yes']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  #remove whitespace\n",
    "    text=re.compile('<.*?>').sub('', text) #remove html markup\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #remove punctuation and special characters\n",
    "    text = re.sub('\\s+', ' ', text)  #remove high spaces and '\\n', '\\r' characters\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) #remove numbers\n",
    "    text = re.sub(r'\\s+',' ',text) #remove high spaces and '\\n', '\\r' characters\n",
    "    return text\n",
    "\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "\n",
    "#print result on an example\n",
    "ex = df['body'][10]\n",
    "print('Joke:\\n', ex)\n",
    "print('-----------------------')\n",
    "print('After removing noisy characters: ', preprocess(ex))\n",
    "print('After removing stopwords: ', stopword(preprocess(ex)))\n",
    "print('Tokenized:', word_tokenize(stopword(preprocess(ex))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3409d98d-ec43-4c44-86be-bae65991e266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>lengths</th>\n",
       "      <th>clean_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3013</td>\n",
       "      <td>Markin' around The Christmas Tree\\nWhat a dogg...</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1022</td>\n",
       "      <td>markin around christmas tree doggie holiday do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12808</td>\n",
       "      <td>Yo mama so fat when jumps up in the air she ge...</td>\n",
       "      <td>Animal</td>\n",
       "      <td>55</td>\n",
       "      <td>yo mama fat jumps air gets stuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11887</td>\n",
       "      <td>Laws of Feline Physics II\\r\\n\\r\\nLaw of Dinner...</td>\n",
       "      <td>Animal</td>\n",
       "      <td>1098</td>\n",
       "      <td>laws feline physics ii law dinner table attend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               body category  lengths  \\\n",
       "0   3013  Markin' around The Christmas Tree\\nWhat a dogg...   Animal     1022   \n",
       "1  12808  Yo mama so fat when jumps up in the air she ge...   Animal       55   \n",
       "2  11887  Laws of Feline Physics II\\r\\n\\r\\nLaw of Dinner...   Animal     1098   \n",
       "\n",
       "                                          clean_body  \n",
       "0  markin around christmas tree doggie holiday do...  \n",
       "1                   yo mama fat jumps air gets stuck  \n",
       "2  laws feline physics ii law dinner table attend...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_body'] = df['body'].apply(lambda x : stopword(preprocess(x)))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6ad8f4-ee13-4dd9-af30-e29f05ca42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe\n",
    "df.to_csv('created_dataframes/df_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470dfd0-118c-43ef-9489-5b4a41a2e396",
   "metadata": {},
   "source": [
    "# Word2Vec model\n",
    "\n",
    "We will use Word2Vec features of the cleaned jokes. This model maps words with similar meaning to similar real-valued vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7fb09f-bccd-47a1-a290-7e52286f8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('created_dataframes/df_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c845e0ca-79bd-495b-8d2c-6ae5c05795ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take clean text of the jokes\n",
    "X = df['clean_body'].values \n",
    "\n",
    "#create document = list of of all words in our data\n",
    "document = []\n",
    "for i in range(len(X)):\n",
    "    joke_tok = nltk.word_tokenize(X[i])\n",
    "    for word in joke_tok:\n",
    "        document.append(word)\n",
    "        \n",
    "document = [document] #create 'list of list' architecture for the word2vec model\n",
    "    \n",
    "#word2vec model\n",
    "SIZE=30 #size of embedding space\n",
    "word2vec_model = Word2Vec(document, min_count=1, size=SIZE, window=2, sg=1, iter=500)\n",
    "\n",
    "#save the model\n",
    "word2vec_model.save(\"word2vec_size30.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89584cfa-bbbd-4390-b4fb-4c405143d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firsts joke tokenized: ['yo', 'mama', 'fat', 'jumps', 'air', 'gets', 'stuck']\n",
      "shape of the embedding: (7, 30)\n",
      "embedding: [[ 1.06863678e+00 -1.05477965e+00 -2.65132487e-01  1.07242644e+00\n",
      "  -1.26709461e-01 -2.04000306e+00  1.55308709e-01 -8.59857559e-01\n",
      "   1.18675566e+00  6.73972070e-01  5.51422477e-01  2.23011225e-01\n",
      "  -6.57179654e-01 -3.48080933e-01 -3.92193526e-01  1.19578242e+00\n",
      "   4.27507132e-01 -1.50509775e+00  9.92367804e-01 -8.62774551e-01\n",
      "   2.76712120e-01 -1.12649727e+00 -1.62747502e+00  8.53952348e-01\n",
      "  -4.48997617e-01  1.16722906e+00 -1.13196731e+00  1.76026309e+00\n",
      "   2.74212646e+00 -1.12593627e+00]\n",
      " [ 5.80063045e-01 -1.52785826e+00 -5.15684128e-01  1.10004830e+00\n",
      "   2.14492023e-01 -1.61776423e+00  2.43962914e-01 -4.28375334e-01\n",
      "   3.42261679e-02  1.24423921e+00  2.09099159e-01  2.76257694e-01\n",
      "  -7.22553432e-01 -5.57646602e-02 -1.04177184e-01  4.22811061e-01\n",
      "   3.30781460e-01 -1.61578417e+00  8.20073426e-01 -4.21348423e-01\n",
      "   3.08500350e-01 -7.21056640e-01 -1.32048702e+00  2.83016026e-01\n",
      "  -1.00455415e+00  7.34656572e-01 -1.31518912e+00  1.33202112e+00\n",
      "   2.53692484e+00 -5.67257941e-01]\n",
      " [ 7.48432219e-01 -1.58027971e+00 -1.09460980e-01  9.13094580e-01\n",
      "   3.35992038e-01 -1.40478313e+00 -6.32715940e-01  1.25638666e-02\n",
      "   1.51954766e-04  1.24054360e+00 -1.50085971e-01 -3.97300035e-01\n",
      "  -9.01333451e-01  5.10664284e-01  2.57131588e-02  3.17956686e-01\n",
      "   5.95717072e-01 -1.35873413e+00  5.89293063e-01 -3.57116789e-01\n",
      "   2.41927490e-01 -3.40295851e-01 -1.03811562e+00  5.17638862e-01\n",
      "  -1.61314547e+00  4.24287111e-01 -6.24814034e-01  1.04017138e+00\n",
      "   3.02131772e+00 -7.71475792e-01]\n",
      " [ 3.42138261e-01 -1.13286686e+00 -8.11115205e-01  9.21029627e-01\n",
      "   3.74529928e-01 -9.56039131e-01  2.00319499e-01  1.09832929e-02\n",
      "  -6.13692939e-01  1.18907213e+00  2.64785767e-01  4.67734009e-01\n",
      "  -7.95723200e-01  8.74230742e-01 -2.00806931e-02  4.00427550e-01\n",
      "   3.94188792e-01 -1.47618222e+00 -2.16397554e-01 -2.57050484e-01\n",
      "   7.64915705e-01 -1.89697772e-01 -4.73867118e-01  1.18912935e+00\n",
      "  -1.90543830e+00  5.94141126e-01 -3.17929119e-01  8.55595350e-01\n",
      "   3.20638680e+00 -1.46833956e+00]\n",
      " [ 5.13845503e-01 -1.46759737e+00 -1.35694337e+00  7.66034663e-01\n",
      "  -3.17518950e-01 -1.93649814e-01 -3.31495523e-01 -4.58280057e-01\n",
      "   9.00039971e-02  1.29872477e+00 -6.05973244e-01 -8.45025241e-01\n",
      "  -7.14487076e-01  8.32475007e-01  3.55670929e-01  2.82580703e-02\n",
      "   6.77610874e-01 -1.36001980e+00  2.62559801e-01 -2.22351015e-01\n",
      "   3.98431838e-01  6.37763977e-01  4.42337506e-02  1.09036088e+00\n",
      "  -1.58448637e+00  9.30731952e-01 -7.80758858e-01  6.32136464e-01\n",
      "   3.16121125e+00 -1.12181330e+00]\n",
      " [ 1.61685610e+00 -1.29487920e+00 -1.23157036e+00  1.67012262e+00\n",
      "  -7.21088409e-01  9.83048007e-02  1.48536003e+00 -1.54057109e+00\n",
      "  -7.89458811e-01  2.80124855e+00  6.01257503e-01 -1.98683357e+00\n",
      "  -2.98316550e+00  7.64916718e-01  3.55910867e-01  1.61980063e-01\n",
      "   2.35127783e+00 -1.40578103e+00 -1.82378680e-01 -1.01062250e+00\n",
      "  -1.89773936e-03 -1.10483110e-01  9.76183236e-01 -9.95822847e-01\n",
      "  -2.74932313e+00 -1.39530122e-01  3.18762630e-01  9.39985812e-01\n",
      "   3.32684088e+00 -1.48390090e+00]\n",
      " [-1.67190880e-01 -3.97762239e-01 -2.00129598e-01  1.12521267e+00\n",
      "  -1.23047411e+00  1.09132923e-01  2.03607261e-01 -9.58789408e-01\n",
      "  -1.09998512e+00  1.99460328e+00 -4.09355909e-01 -1.50093928e-01\n",
      "  -1.03099048e+00  5.81105292e-01  3.65595520e-02 -2.79362887e-01\n",
      "   2.60312051e-01 -1.33536220e+00  7.10804701e-01  7.92448372e-02\n",
      "   1.19923449e+00  1.00320652e-01  3.87531370e-01  6.68870389e-01\n",
      "  -2.95313418e-01  5.83726168e-01 -1.27400601e+00  4.84202683e-01\n",
      "   2.76783395e+00 -2.16561389e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize cleaned jokes\n",
    "cleaned_jokes = df['clean_body'].values \n",
    "jokes_tok = [nltk.word_tokenize(i) for i in cleaned_jokes]\n",
    "print('firsts joke tokenized:', jokes_tok[1]) #first joke tokenized\n",
    "\n",
    "#print word2vec embeddings\n",
    "word2vec_model = Word2Vec.load(\"word2vec_size30.model\")\n",
    "print('shape of the embedding:', word2vec_model.wv[jokes_tok[1]].shape)\n",
    "print('embedding:', word2vec_model.wv[jokes_tok[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f703c-3637-420d-8565-6ff5670410f9",
   "metadata": {},
   "source": [
    "The Word2Vec model maps each word to a vector that has the size of the embedding space (specified when creating the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2504c1-f67a-439b-86bb-c3f34caf6530",
   "metadata": {},
   "source": [
    "# Create X and Y\n",
    "\n",
    "## Create X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a172885-1a60-41a9-bb2b-41b85ea7516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 6000)\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings(cleaned_jokes, size_embed=30):\n",
    "    '''\n",
    "    Create embeddings of jokes\n",
    "    Args:\n",
    "        -cleaned_jokes: list of cleaned jokes\n",
    "    Returns:\n",
    "        -embeddings: Concatenated (zero padded) Word embeddings of each joke\n",
    "    '''\n",
    "    #load the exisiting Word2Vec model\n",
    "    word2vec_model = Word2Vec.load(\"word2vec_size\"+str(size_embed)+\".model\")\n",
    "    \n",
    "    MAX_NB_WORDS = 200 #maximum number of words considered for each joke\n",
    "    embeddings = []\n",
    "    \n",
    "    #tokenize the jokes\n",
    "    jokes_tok = [nltk.word_tokenize(joke) for joke in cleaned_jokes]\n",
    "    \n",
    "    #we will compute the embedding of joke\n",
    "    for joke in jokes_tok:\n",
    "        embedding_joke = np.array([])\n",
    "        \n",
    "        #we will compute the embedding of each word, and concatenate the result to a array so that we have only one array representing a joke\n",
    "        count_words=0\n",
    "        for word in joke:\n",
    "            #if the word2vec model never encountered the word, remove it\n",
    "            if word not in word2vec_model.wv.vocab.keys():\n",
    "                continue\n",
    "                \n",
    "            if count_words==MAX_NB_WORDS: #if we exceeded the total number of words, we stop\n",
    "                break\n",
    "            if embedding_joke.shape[0] == 0: #First word in the joke: embedding_joke is empty\n",
    "                embedding_joke = word2vec_model.wv[word]\n",
    "            else:\n",
    "                embedding_joke = np.concatenate([embedding_joke, word2vec_model.wv[word]], axis=0)\n",
    "            count_words+=1\n",
    "                \n",
    "        #test if the number of words is inferior to the number of total words\n",
    "        if embedding_joke.shape[0] < MAX_NB_WORDS*size_embed: #embedding space has a size of 15\n",
    "            nb_of_values_to_add = MAX_NB_WORDS*size_embed - embedding_joke.shape[0] #number of values to add to get the same shape as others\n",
    "            embedding_joke = np.pad(embedding_joke, (0, nb_of_values_to_add), constant_values=0.)\n",
    "        \n",
    "        #add to array of embeddings\n",
    "        embeddings.append(embedding_joke)\n",
    "        \n",
    "        \n",
    "    return np.array(embeddings)\n",
    "\n",
    "X = create_embeddings(df['clean_body'].values)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55de47-7300-4294-9795-ff6c24d38752",
   "metadata": {},
   "source": [
    "## Create Y\n",
    "\n",
    "In this section, we convert the category attribute (string) into a one-hot vector, for example:\n",
    "\n",
    "- 'Animal' will be equal to [1, 0, 0, 0 ..., 0]\n",
    "- 'Bar' will be equal to [0, 0, 1, 0, ..., 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1388cf-a9a3-45e0-9301-b9e53abbc6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2238 1129 1561 2162 1160]\n",
      "Random 5 categories:\n",
      " ['Men / Women' 'One Liners' 'Sports' 'Men / Women' 'One Liners']\n",
      "Corresponding 5 one-hot encoded categories:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "y = pd.get_dummies(df['category']).values\n",
    "x = np.random.randint(0, 3300, size=5)\n",
    "print(x)\n",
    "print('Random 5 categories:\\n', df.iloc[x]['category'].values)\n",
    "print('Corresponding 5 one-hot encoded categories:\\n', y[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00ddf3f-7e4c-4d90-842f-a4e7a3cf1de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 23)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984984df-1751-4286-8eb2-4aa1ad113832",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12815215-1af6-402c-9bdc-2fdf43b31f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5520, 6000) (5520, 23)\n",
      "(1380, 6000) (1380, 23)\n"
     ]
    }
   ],
   "source": [
    "#split train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48d3b5b4-8627-45c7-9761-8a7275e2c026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/10, Iteration : 500/5520 , Loss : 0.041615374087690116\n",
      "Epoch : 1/10, Iteration : 1000/5520 , Loss : 0.04111515875450356\n",
      "Epoch : 1/10, Iteration : 1500/5520 , Loss : 0.04009425029596491\n",
      "Epoch : 1/10, Iteration : 2000/5520 , Loss : 0.039275629965230104\n",
      "Epoch : 1/10, Iteration : 2500/5520 , Loss : 0.03816463646472185\n",
      "Epoch : 1/10, Iteration : 3000/5520 , Loss : 0.03642680443838385\n",
      "Epoch : 1/10, Iteration : 3500/5520 , Loss : 0.036217531269897504\n",
      "Epoch : 1/10, Iteration : 4000/5520 , Loss : 0.03722091781337859\n",
      "Epoch : 1/10, Iteration : 4500/5520 , Loss : 0.035098847669987096\n",
      "Epoch : 1/10, Iteration : 5000/5520 , Loss : 0.03536662477919511\n",
      "Epoch : 1/10, Iteration : 5500/5520 , Loss : 0.033409736459906705\n",
      "Epoch : 2/10, Iteration : 500/5520 , Loss : 0.031311861837943315\n",
      "Epoch : 2/10, Iteration : 1000/5520 , Loss : 0.0307268901730926\n",
      "Epoch : 2/10, Iteration : 1500/5520 , Loss : 0.031022331491526813\n",
      "Epoch : 2/10, Iteration : 2000/5520 , Loss : 0.029508511836517967\n",
      "Epoch : 2/10, Iteration : 2500/5520 , Loss : 0.02890057085491634\n",
      "Epoch : 2/10, Iteration : 3000/5520 , Loss : 0.027391702100408977\n",
      "Epoch : 2/10, Iteration : 3500/5520 , Loss : 0.026748370562143714\n",
      "Epoch : 2/10, Iteration : 4000/5520 , Loss : 0.028086791164837734\n",
      "Epoch : 2/10, Iteration : 4500/5520 , Loss : 0.025120834364373994\n",
      "Epoch : 2/10, Iteration : 5000/5520 , Loss : 0.026004392720854107\n",
      "Epoch : 2/10, Iteration : 5500/5520 , Loss : 0.024553158048881172\n",
      "Epoch : 3/10, Iteration : 500/5520 , Loss : 0.02155506398698917\n",
      "Epoch : 3/10, Iteration : 1000/5520 , Loss : 0.020163009945258225\n",
      "Epoch : 3/10, Iteration : 1500/5520 , Loss : 0.02051185400326783\n",
      "Epoch : 3/10, Iteration : 2000/5520 , Loss : 0.020157119331017243\n",
      "Epoch : 3/10, Iteration : 2500/5520 , Loss : 0.019701893029097285\n",
      "Epoch : 3/10, Iteration : 3000/5520 , Loss : 0.018761016676922145\n",
      "Epoch : 3/10, Iteration : 3500/5520 , Loss : 0.01793985199142508\n",
      "Epoch : 3/10, Iteration : 4000/5520 , Loss : 0.018685381489072214\n",
      "Epoch : 3/10, Iteration : 4500/5520 , Loss : 0.016588410464163497\n",
      "Epoch : 3/10, Iteration : 5000/5520 , Loss : 0.017297797298128362\n",
      "Epoch : 3/10, Iteration : 5500/5520 , Loss : 0.016562947778593407\n",
      "Epoch : 4/10, Iteration : 500/5520 , Loss : 0.014352146939895925\n",
      "Epoch : 4/10, Iteration : 1000/5520 , Loss : 0.012933084761158682\n",
      "Epoch : 4/10, Iteration : 1500/5520 , Loss : 0.013430440756925958\n",
      "Epoch : 4/10, Iteration : 2000/5520 , Loss : 0.013328017031367946\n",
      "Epoch : 4/10, Iteration : 2500/5520 , Loss : 0.012945493202424333\n",
      "Epoch : 4/10, Iteration : 3000/5520 , Loss : 0.012846057743991949\n",
      "Epoch : 4/10, Iteration : 3500/5520 , Loss : 0.01185954175744114\n",
      "Epoch : 4/10, Iteration : 4000/5520 , Loss : 0.012318186101991839\n",
      "Epoch : 4/10, Iteration : 4500/5520 , Loss : 0.010916251921741149\n",
      "Epoch : 4/10, Iteration : 5000/5520 , Loss : 0.011939984912970544\n",
      "Epoch : 4/10, Iteration : 5500/5520 , Loss : 0.010800619795905912\n",
      "Epoch : 5/10, Iteration : 500/5520 , Loss : 0.009984678827047209\n",
      "Epoch : 5/10, Iteration : 1000/5520 , Loss : 0.00870250667991668\n",
      "Epoch : 5/10, Iteration : 1500/5520 , Loss : 0.009710180688943044\n",
      "Epoch : 5/10, Iteration : 2000/5520 , Loss : 0.009527388956781376\n",
      "Epoch : 5/10, Iteration : 2500/5520 , Loss : 0.00912212986584618\n",
      "Epoch : 5/10, Iteration : 3000/5520 , Loss : 0.009121163256004748\n",
      "Epoch : 5/10, Iteration : 3500/5520 , Loss : 0.008246871793924146\n",
      "Epoch : 5/10, Iteration : 4000/5520 , Loss : 0.008865216477746575\n",
      "Epoch : 5/10, Iteration : 4500/5520 , Loss : 0.007410512102543047\n",
      "Epoch : 5/10, Iteration : 5000/5520 , Loss : 0.008813968561373151\n",
      "Epoch : 5/10, Iteration : 5500/5520 , Loss : 0.007371195604942492\n",
      "Epoch : 6/10, Iteration : 500/5520 , Loss : 0.007354381870880483\n",
      "Epoch : 6/10, Iteration : 1000/5520 , Loss : 0.00636717452306549\n",
      "Epoch : 6/10, Iteration : 1500/5520 , Loss : 0.0073175175839081015\n",
      "Epoch : 6/10, Iteration : 2000/5520 , Loss : 0.007253218938700542\n",
      "Epoch : 6/10, Iteration : 2500/5520 , Loss : 0.006663541755036408\n",
      "Epoch : 6/10, Iteration : 3000/5520 , Loss : 0.00677016329360809\n",
      "Epoch : 6/10, Iteration : 3500/5520 , Loss : 0.006110406281386638\n",
      "Epoch : 6/10, Iteration : 4000/5520 , Loss : 0.006921737930928781\n",
      "Epoch : 6/10, Iteration : 4500/5520 , Loss : 0.005757440100169049\n",
      "Epoch : 6/10, Iteration : 5000/5520 , Loss : 0.006861652256515821\n",
      "Epoch : 6/10, Iteration : 5500/5520 , Loss : 0.005522662517887491\n",
      "Epoch : 7/10, Iteration : 500/5520 , Loss : 0.005796426933823494\n",
      "Epoch : 7/10, Iteration : 1000/5520 , Loss : 0.004951094201613322\n",
      "Epoch : 7/10, Iteration : 1500/5520 , Loss : 0.005539207038510414\n",
      "Epoch : 7/10, Iteration : 2000/5520 , Loss : 0.005493337780940479\n",
      "Epoch : 7/10, Iteration : 2500/5520 , Loss : 0.005153401324620989\n",
      "Epoch : 7/10, Iteration : 3000/5520 , Loss : 0.005249250965697107\n",
      "Epoch : 7/10, Iteration : 3500/5520 , Loss : 0.004470264190784261\n",
      "Epoch : 7/10, Iteration : 4000/5520 , Loss : 0.005368887530375745\n",
      "Epoch : 7/10, Iteration : 4500/5520 , Loss : 0.004638423779163836\n",
      "Epoch : 7/10, Iteration : 5000/5520 , Loss : 0.005384263655892266\n",
      "Epoch : 7/10, Iteration : 5500/5520 , Loss : 0.004311704622945303\n",
      "Epoch : 8/10, Iteration : 500/5520 , Loss : 0.004613593890461059\n",
      "Epoch : 8/10, Iteration : 1000/5520 , Loss : 0.003820633311277863\n",
      "Epoch : 8/10, Iteration : 1500/5520 , Loss : 0.004335144778947182\n",
      "Epoch : 8/10, Iteration : 2000/5520 , Loss : 0.004446838581176043\n",
      "Epoch : 8/10, Iteration : 2500/5520 , Loss : 0.004130395463127827\n",
      "Epoch : 8/10, Iteration : 3000/5520 , Loss : 0.00394164589447696\n",
      "Epoch : 8/10, Iteration : 3500/5520 , Loss : 0.003408140134022907\n",
      "Epoch : 8/10, Iteration : 4000/5520 , Loss : 0.004049006681191384\n",
      "Epoch : 8/10, Iteration : 4500/5520 , Loss : 0.003506119386681589\n",
      "Epoch : 8/10, Iteration : 5000/5520 , Loss : 0.004499791016027854\n",
      "Epoch : 8/10, Iteration : 5500/5520 , Loss : 0.003497425466537479\n",
      "Epoch : 9/10, Iteration : 500/5520 , Loss : 0.003502947962960753\n",
      "Epoch : 9/10, Iteration : 1000/5520 , Loss : 0.0031026737624106\n",
      "Epoch : 9/10, Iteration : 1500/5520 , Loss : 0.003288512728506394\n",
      "Epoch : 9/10, Iteration : 2000/5520 , Loss : 0.003659323019824745\n",
      "Epoch : 9/10, Iteration : 2500/5520 , Loss : 0.003114199167352807\n",
      "Epoch : 9/10, Iteration : 3000/5520 , Loss : 0.0032210971463158974\n",
      "Epoch : 9/10, Iteration : 3500/5520 , Loss : 0.002942674719318095\n",
      "Epoch : 9/10, Iteration : 4000/5520 , Loss : 0.003503480393347562\n",
      "Epoch : 9/10, Iteration : 4500/5520 , Loss : 0.0024789798911985256\n",
      "Epoch : 9/10, Iteration : 5000/5520 , Loss : 0.0037932097291444706\n",
      "Epoch : 9/10, Iteration : 5500/5520 , Loss : 0.0029764408401296532\n",
      "Epoch : 10/10, Iteration : 500/5520 , Loss : 0.0028444067579059917\n",
      "Epoch : 10/10, Iteration : 1000/5520 , Loss : 0.002577564044907276\n",
      "Epoch : 10/10, Iteration : 1500/5520 , Loss : 0.002667255425580527\n",
      "Epoch : 10/10, Iteration : 2000/5520 , Loss : 0.0028179630408977866\n",
      "Epoch : 10/10, Iteration : 2500/5520 , Loss : 0.002551192541270922\n",
      "Epoch : 10/10, Iteration : 3000/5520 , Loss : 0.002805006642997994\n",
      "Epoch : 10/10, Iteration : 3500/5520 , Loss : 0.0024058756510174817\n",
      "Epoch : 10/10, Iteration : 4000/5520 , Loss : 0.003071592632992459\n",
      "Epoch : 10/10, Iteration : 4500/5520 , Loss : 0.0017229523625556372\n",
      "Epoch : 10/10, Iteration : 5000/5520 , Loss : 0.0029331682802709153\n",
      "Epoch : 10/10, Iteration : 5500/5520 , Loss : 0.0026893854445732552\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "from model_embed import LSTM_embed\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "HIDDEN_DIM = 128\n",
    "output_size = y.shape[1] #number of categories\n",
    "\n",
    "model = LSTM_embed(input_size, HIDDEN_DIM, output_size)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10\n",
    "\n",
    "model.learn(X_train, Y_train, loss_function, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb6f80e-d428-4c7d-a2a3-ce8e5a8000ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Animal       0.38      0.39      0.39        56\n",
      "          Bar       0.70      0.81      0.75        52\n",
      "       Blonde       0.53      0.34      0.42        58\n",
      "     Business       0.59      0.43      0.49        56\n",
      "     Children       0.54      0.53      0.53        59\n",
      "      College       0.80      0.80      0.80        55\n",
      "        Gross       0.73      0.57      0.64        63\n",
      "      Insults       0.49      0.42      0.45        55\n",
      "  Knock-Knock       1.00      0.95      0.98        64\n",
      "      Lawyers       0.55      0.69      0.61        58\n",
      "    Lightbulb       0.94      0.96      0.95        68\n",
      "      Medical       0.57      0.57      0.57        63\n",
      "  Men / Women       0.25      0.28      0.26        53\n",
      "Miscellaneous       0.30      0.34      0.32        62\n",
      "   One Liners       0.42      0.42      0.42        64\n",
      " Other / Misc       0.11      0.17      0.13        47\n",
      "    Political       0.60      0.51      0.55        69\n",
      "         Puns       0.26      0.37      0.31        59\n",
      "      Redneck       0.70      0.61      0.66        62\n",
      "    Religious       0.41      0.38      0.39        69\n",
      "       Sports       0.71      0.70      0.71        71\n",
      "         Tech       0.67      0.54      0.60        48\n",
      "      Yo Mama       0.91      0.97      0.94        69\n",
      "\n",
      "    micro avg       0.56      0.56      0.56      1380\n",
      "    macro avg       0.57      0.55      0.56      1380\n",
      " weighted avg       0.58      0.56      0.57      1380\n",
      "  samples avg       0.56      0.56      0.56      1380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.compute_predictions(X_test)\n",
    "\n",
    "labels = np.sort(df['category'].unique())\n",
    "print(classification_report(Y_test, predictions, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842878b5-a904-4a7d-8fbe-0f67c7fd5547",
   "metadata": {},
   "source": [
    "We see that some categories are more spotable than others. For example the 'yo Mama' jokes are well classified, whereas the 'Men / Women' jokes are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1fcbfcc-5540-4ad6-a27b-dc2933c1f1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 0.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def one_hot_to_category(encoded_y):\n",
    "    '''\n",
    "    Converts the one-hot prediction into the string category\n",
    "    '''\n",
    "    df = pd.read_csv('created_dataframes/df_preprocessed.csv')\n",
    "    categories = np.sort(df['category'].unique())\n",
    "    labels=[]\n",
    "    if len(encoded_y.shape)!=1:\n",
    "        for y in encoded_y:\n",
    "            index = np.argmax(y) \n",
    "            label = categories[index]\n",
    "            labels.append(label)\n",
    "    else:\n",
    "        index = np.argmax(encoded_y) \n",
    "        label = categories[index]\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def accuracy(predictions, true_values):\n",
    "    '''Computes accuracy'''\n",
    "    count_misclassified = 0\n",
    "    for i in range(len(predictions)):\n",
    "        real_category = one_hot_to_category(true_values[i])\n",
    "        predicted_category = one_hot_to_category(predictions[i])\n",
    "        if real_category!=predicted_category:\n",
    "            count_misclassified+=1\n",
    "    return 1 - count_misclassified/len(predictions)\n",
    "\n",
    "print('Global Accuracy: {:.2f}\\n'.format(accuracy(predictions, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049abb73-5941-43cf-b8b0-469efc9b1fa1",
   "metadata": {},
   "source": [
    "The global accuracy is not great. One way to improve the performance would be to improve the model, we could for example use state-of-the-art text classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e575404-3e17-4c01-8cb3-b2f8f209711c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
